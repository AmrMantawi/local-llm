# Local LLM

A complete local AI assistant with Speech-to-Text (STT), Large Language Model (LLM), and Text-to-Speech (TTS) capabilities. Features a high-performance multi-threaded async pipeline architecture for real-time voice interaction.

## ğŸš€ Features

- **Real-time Voice Assistant**: Full Audio â†’ STT â†’ LLM â†’ TTS pipeline
- **Multi-threaded Architecture**: Parallel processing with thread-safe queues
- **Multiple Pipeline Modes**: Voice assistant, text-only, transcription, synthesis
- **Speech-to-Text**: Whisper-based transcription with Voice Activity Detection (VAD)
- **Language Model**: Llama-based text generation with GGUF model support
- **Text-to-Speech**: Paroli TTS with ONNX neural voice synthesis
- **Server Mode**: Unix socket server for integration with other applications
- **Cross-platform**: Linux, Windows (WSL)
- **Privacy-first**: Everything runs locally, no cloud dependencies
- **Performance Optimized**: Release builds with -O3 optimizations and native CPU targeting

## ğŸ“‹ Requirements

### System Dependencies
- **CMake** 3.16+
- **SDL2** development libraries
- **Python** 3.7+
- **ALSA** (Linux)
- **ONNX Runtime** (automatically downloaded)
- **nlohmann/json** (automatically downloaded)

### Ubuntu/Debian
```bash
sudo apt-get update
sudo apt-get install cmake libsdl2-dev alsa-utils
```

## ğŸ› ï¸ Quick Setup

1. **Clone the repository:**
   ```bash
   git clone https://github.com/amrmantawi/local-llm.git
   cd local-llm
   ```

2. **Run the setup script:**
   ```bash
   make setup
   ```

3. **Download models:**
   - **STT**: Download Whisper models from [Hugging Face](https://huggingface.co/ggerganov/whisper.cpp)
   - **LLM**: Download GGUF models from [Hugging Face](https://huggingface.co/models?search=gguf)
   - **TTS**: Download Paroli models (encoder.onnx, decoder.onnx, config.json) from [Paroli releases](https://github.com/paroli-ai/paroli-daemon/releases)

4. **Place models in the correct directories:**
   ```
   models/
   â”œâ”€â”€ stt/     # .bin files (Whisper)
   â”œâ”€â”€ llm/     # .gguf files (Llama)
   â””â”€â”€ tts/     # .onnx files (Paroli) + config.json + espeak-ng-data/
   ```

5. **Build and run:**
   ```bash
   # Build optimized release version
   make build
   
   # Run in CLI mode (voice assistant)
   make run
   
   # Or run server mode
   ./build/local-llm --server --socket /run/local-llm.sock
   ```

## ğŸ¯ Usage

### CLI Mode (Voice Assistant)
Interactive voice chat with real-time processing:
1. **Start the application** - initializes all components in parallel threads
2. **Speak into your microphone** - VAD automatically detects speech
3. **Wait for transcription** - Whisper converts speech to text
4. **Get AI response** - Llama generates contextual response
5. **Hear the response** - Paroli synthesizes and plays audio

### Server Mode
Unix socket server for integration with other applications:
```bash
./build/local-llm --server --socket /run/local-llm.sock
```

### Command Line Options
```bash
./build/local-llm [options]

Options:
  --server              Run in server mode (default: CLI mode)
  --config PATH         Path to models.json config file
  --socket PATH         Unix socket path for server mode
  --help, -h            Show help message
```

**Controls:**
- Press `Ctrl+C` to exit gracefully (proper cleanup of all threads and audio devices)

## ğŸ—ï¸ Architecture

### Multi-threaded Pipeline
The system uses a sophisticated async pipeline architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STTProcessor   â”‚â”€â”€â”€â–¶â”‚  LLMProcessor   â”‚â”€â”€â”€â–¶â”‚  TTSProcessor   â”‚â”€â”€â”€â–¶â”‚AudioOutputProc â”‚
â”‚  (Audioâ†’Text)   â”‚    â”‚  (Textâ†’Text)    â”‚    â”‚  (Textâ†’Audio)   â”‚    â”‚  (Audio Play)  â”‚
â”‚  SDL2 + Whisper â”‚    â”‚  Llama.cpp      â”‚    â”‚  Paroli TTS     â”‚    â”‚  ALSA Output   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline Modes
- **VOICE_ASSISTANT**: Full Audio â†’ STT â†’ LLM â†’ TTS chain
- **TEXT_ONLY**: LLM only for server mode
- **TRANSCRIPTION**: Audio â†’ STT â†’ Text
- **SYNTHESIS**: Text â†’ TTS â†’ Audio

### Performance Optimizations
- **Thread-safe Queues**: Bounded queues with interrupt support
- **Signal-based Control**: Immediate interruption and graceful shutdown
- **Memory Management**: RAII with smart pointers, move semantics
- **Audio Optimization**: Low-latency ALSA with immediate interruption
- **GPU Support**: Llama supports GPU layer offloading
- **Release Optimizations**: -O3, -march=native, -DNDEBUG by default

## ğŸ“ Project Structure

```
local-llm/
â”œâ”€â”€ include/                    # Header files
â”‚   â”œâ”€â”€ async_pipeline.h        # Core async pipeline framework
â”‚   â”œâ”€â”€ async_processors.h      # Processor implementations
â”‚   â”œâ”€â”€ pipeline_manager.h      # Pipeline coordination
â”‚   â”œâ”€â”€ async_pipeline_factory.h # Factory for pipeline creation
â”‚   â”œâ”€â”€ stt.h                   # STT interface
â”‚   â”œâ”€â”€ llm.h                   # LLM interface
â”‚   â”œâ”€â”€ tts.h                   # TTS interface
â”‚   â”œâ”€â”€ stt_whisper.h           # Whisper STT implementation
â”‚   â”œâ”€â”€ llm_llama.h             # Llama LLM implementation
â”‚   â”œâ”€â”€ tts_paroli.h            # Paroli TTS implementation
â”‚   â””â”€â”€ config_manager.h        # Configuration management
â”œâ”€â”€ src/                        # Source files
â”‚   â”œâ”€â”€ main.cpp                # Main application entry point
â”‚   â”œâ”€â”€ async_pipeline_factory.cpp # Pipeline factory implementation
â”‚   â”œâ”€â”€ async_processors.cpp    # Processor implementations
â”‚   â”œâ”€â”€ stt_whisper.cpp         # Whisper STT backend
â”‚   â”œâ”€â”€ llm_llama.cpp           # Llama LLM backend
â”‚   â”œâ”€â”€ tts_paroli.cpp          # Paroli TTS backend
â”‚   â”œâ”€â”€ common.cpp              # Utility functions
â”‚   â””â”€â”€ common-sdl.cpp          # SDL audio utilities
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â””â”€â”€ setup.sh                # Setup script
â”œâ”€â”€ config/                     # Configuration files
â”‚   â””â”€â”€ models.json             # Model paths and settings
â”œâ”€â”€ models/                     # Model files (not in git)
â”‚   â”œâ”€â”€ llm/                    # GGUF language models
â”‚   â”œâ”€â”€ tts/                    # TTS ONNX models + config
â”‚   â””â”€â”€ stt/                    # Whisper models
â”œâ”€â”€ third_party/                # External dependencies
â”‚   â”œâ”€â”€ llama.cpp/              # Llama.cpp library
â”‚   â””â”€â”€ paroli-daemon/          # Paroli TTS library
â”œâ”€â”€ deps/                       # Downloaded dependencies
â”‚   â”œâ”€â”€ onnxruntime-*/          # ONNX Runtime
â”‚   â””â”€â”€ piper_phonemize/        # Piper phonemization
â”œâ”€â”€ CMakeLists.txt              # CMake build configuration
â”œâ”€â”€ Makefile                    # Convenient build targets
â””â”€â”€ build/                      # Build artifacts (not in git)
```

## âš™ï¸ Configuration

### Configuration File
Edit `config/models.json` to customize:
- Model file paths
- Audio settings (sample rate, buffer size, VAD parameters)
- LLM parameters (context size, GPU layers, sampling)
- TTS voice settings

### Build Configuration
The project uses CMake with the following options:

**Backend Selection:**
- `-DUSE_WHISPER=ON` - Enable Whisper STT (default)
- `-DUSE_LLAMA=ON` - Enable Llama LLM (default)
- `-DUSE_Paroli=ON` - Enable Paroli TTS (default)
- `-DUSE_RKLLM=ON` - Enable RKLLM for embedded devices

**Build Options:**
- `-DCMAKE_BUILD_TYPE=Release` - Optimized release build (default)
- `-DCMAKE_BUILD_TYPE=Debug` - Debug build with symbols
- `-DENABLE_STATS_LOGGING=ON` - Enable performance statistics

## ğŸ”§ Development

### Building
```bash
# Build optimized release version (default)
make build

# Build debug version with stats
make debug

# Clean build artifacts
make clean

# Show all available targets
make help
```

### Manual CMake Build
```bash
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release ..
make -j$(nproc)
```

### Performance Profiling
```bash
# Build with statistics logging
make debug

# Run with profiling
./build/local-llm
# Statistics will be printed on shutdown
```

### Adding New Backends
1. Create interface header in `include/` (e.g., `stt_new.h`)
2. Implement backend class in `src/` (e.g., `stt_new.cpp`)
3. Add CMake option in `CMakeLists.txt`
4. Update `async_pipeline_factory.cpp` to include new backend
5. Add conditional compilation with `#ifdef USE_NEW_BACKEND`

## ğŸ› Troubleshooting

### Common Issues

**Audio Issues:**
- Ensure ALSA is properly configured: `aplay -l`
- Check microphone permissions
- Try different audio devices in config

**Performance Issues:**
- Enable GPU acceleration in Llama config
- Use optimized models (quantized GGUF)
- Build in Release mode: `make build`

**Build Issues:**
- Ensure all dependencies are installed
- Clean build: `make clean && make build`
- Check CMake version (3.16+ required)

### Debug Mode
```bash
# Build with debug symbols and stats
make debug

# Run with verbose output
./build/local-llm --config config/models.json
```

## ğŸ“Š Performance

### Optimizations Included
- **Compiler**: -O3 optimizations with native CPU targeting
- **Threading**: Parallel processing with hardware-concurrency awareness
- **Memory**: RAII, move semantics, smart pointers
- **Audio**: Low-latency ALSA with immediate interruption
- **GPU**: Optional GPU acceleration for LLM inference

### Expected Performance
- **Audio Latency**: <100ms end-to-end
- **STT Processing**: Real-time transcription
- **LLM Generation**: Depends on model size and hardware
- **TTS Synthesis**: Real-time audio generation

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes
4. Test with both debug and release builds
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [whisper.cpp](https://github.com/ggerganov/whisper.cpp) for STT
- [llama.cpp](https://github.com/ggerganov/llama.cpp) for LLM
- [Paroli](https://github.com/paroli-ai/paroli-daemon) for TTS
- [ONNX Runtime](https://github.com/microsoft/onnxruntime) for TTS acceleration